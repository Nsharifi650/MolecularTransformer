{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "file_handler = logging.FileHandler('Logs.log')\n",
    "console_handler = logging.StreamHandler()\n",
    "file_handler.setLevel(logging.DEBUG)\n",
    "console_handler.setLevel(logging.ERROR)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s - %(message)s - Line: %(lineno)d', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "file_handler.setFormatter(formatter)\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(file_handler)\n",
    "logger.addHandler(console_handler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MULTIHEAD ATTENTION \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        try:\n",
    "            assert d_model % num_heads == 0\n",
    "        except Exception as e:\n",
    "            logger.error(\"dimension of the embedding model is not divisable by number of heads\")\n",
    "        \n",
    "        self.d_models = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = d_model // num_heads\n",
    "\n",
    "        # The query, key, value learnable matrices\n",
    "        self.Wq = nn.Linear(d_model, d_model)\n",
    "        self.Wk = nn.Linear(d_model, d_model)\n",
    "        self.Wv = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.FCLayer = nn.Linear(d_model, d_model)\n",
    "    def split_embedding_perHead(self,x):\n",
    "        # x shape is (batch_size, seq_len, d_model)\n",
    "        (batch_size, seq_len, d_model) = x.shape\n",
    "        # logger.info(f\"multi-head; x-shape: {x.shape}\")\n",
    "        # let's reshape to (batch_size, seq_len, num_heads, depth)\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.depth)\n",
    "        logger.info(f\"Multi-head; x reshaped: {x.shape} \")\n",
    "        # changing the dimensions order to:(batch_size, num_heads, seq_len, depth)\n",
    "        x = x.permute(0,2,1,3)\n",
    "        return x\n",
    "    \n",
    "    def cal_attention(self,q,k,v,mask):\n",
    "        qk = torch.matmul(q, k.permute(0,1,3,2))\n",
    "        dk=torch.tensor(k.shape[-1], dtype=torch.float32)\n",
    "        #dk is a tensor scalar!\n",
    "        attention = qk/torch.sqrt(dk)\n",
    "\n",
    "        # print(\"CHECKING PADDING MASK CODE\")\n",
    "        # print(\"q shape\", q.shape)\n",
    "        # print(\"attention shape\", attention.shape)\n",
    "        if mask is not None:\n",
    "            attention += (mask*-1e9)\n",
    "        # print(\"attention values after masking\", attention[0,0,:,:])\n",
    "        attention_weights = F.softmax(attention, dim=-1) # should be applied along the sequence which is the 3rd dimension\n",
    "        output = torch.matmul(attention_weights, v)\n",
    "\n",
    "\n",
    "        # print(\"mask\", mask[0,0,:,:])\n",
    "        # print(\"q shape\", q.shape)\n",
    "        # print(\"mask shape\", mask.shape)\n",
    "        # print(\"attention weights shape\", attention_weights.shape)\n",
    "        # print(\"attention weights example\", attention_weights[0,0,:,:])\n",
    "        return output, attention_weights\n",
    "    \n",
    "    def forward(self, v,k,q,mask):\n",
    "        batch_size = q.shape[0]\n",
    "        # shapes for debugging\n",
    "        # print(\"v shape\", v.shape)\n",
    "        # print(\"q shape\", q.shape)\n",
    "        # print(\"k shape\", k.shape)\n",
    "        # print(\"mask shape\", mask.shape)\n",
    "        q = self.split_embedding_perHead(self.Wq(q))\n",
    "        k = self.split_embedding_perHead(self.Wk(k))\n",
    "        v = self.split_embedding_perHead(self.Wv(v))\n",
    "\n",
    "        # print(\"v shape after splitting\", v.shape)\n",
    "        # print(\"q shape after splitting\", q.shape)\n",
    "        # print(\"k shape after splitting\", k.shape)\n",
    "\n",
    "        attention,atten_weights = self.cal_attention(q,k,v,mask)\n",
    "        attention = attention.permute(0,2,1,3).contiguous()\n",
    "        attention = attention.reshape(batch_size, -1, self.d_models)\n",
    "\n",
    "        output = self.FCLayer(attention)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE ENCODER LAYER\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self,d_model,dff):\n",
    "        super(EncoderLayer,self).__init__()\n",
    "        self.FeedForwardNN = nn.Sequential(\n",
    "            nn.Linear(d_model,dff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dff,dff)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        output = self.FeedForwardNN(x)\n",
    "        logger.info(f\"encoder output dimensions {output.shape}\")\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE DECODER LAYER\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self,d_model, num_heads, dff):\n",
    "        super(DecoderLayer,self).__init__()\n",
    "        self.MultiHAttention1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.MultiHAttention2 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.FeedForwardNN = nn.Sequential(\n",
    "            nn.Linear(d_model,dff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dff,d_model)\n",
    "\n",
    "        )\n",
    "        self.layerNorm1 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.layerNorm2 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.layerNorm3 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "    def forward(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "        # print(f\"FIRST MHA WITH LOOK AHEAD MASK\")\n",
    "        attn_output1 = self.MultiHAttention1(x,x,x,look_ahead_mask)\n",
    "        attn_output1 = self.layerNorm1(x+attn_output1)\n",
    "        # print(f\"decoder input into second multihead attention layer:{attn_output1.shape}\")\n",
    "        attn_output2 = self.MultiHAttention2(enc_output, enc_output,attn_output1, padding_mask)\n",
    "        attn_output2 = self.layerNorm2(attn_output2+attn_output1)\n",
    "\n",
    "        Feedforward_output = self.FeedForwardNN(attn_output2)\n",
    "        final_output = self.layerNorm3(attn_output2+Feedforward_output)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(target_vocab_size, d_model) # d_model is the size of embedding vector\n",
    "        self.pos_encoding = self.positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, dff) for _ in range(num_layers)])\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(np.arange(position)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model)\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "        pos_encoding = angle_rads[np.newaxis, ...]\n",
    "        return torch.tensor(pos_encoding, dtype=torch.float32)\n",
    "\n",
    "    def get_angles(self, pos, i, d_model):\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "        return pos * angle_rates\n",
    "\n",
    "    def forward(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "        logger.info(f\"decoder input shape to the embedding: {x.shape}\")\n",
    "        seq_len = x.size(1)\n",
    "        x = self.embedding(x)\n",
    "        logger.info(f\"decoder input shape after embedding: {x.shape}\")\n",
    "        x *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i](x, enc_output, look_ahead_mask, padding_mask)\n",
    "\n",
    "        logger.info(f\"final decoder output shape {x.shape}\")\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORMER\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self,num_layers, enc_d_model, dec_d_model,\n",
    "                enc_num_heads, dec_num_heads, enc_dff, \n",
    "                dec_dff, target_vocab_size, pe_target):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size)\n",
    "        self.encoder = EncoderLayer(enc_d_model, enc_dff)\n",
    "        self.decoder = Decoder(num_layers, dec_d_model, dec_num_heads, dec_dff, target_vocab_size, pe_target)\n",
    "        self.final_layer = nn.Linear(dec_d_model, target_vocab_size)\n",
    "\n",
    "        \n",
    "    def forward(self, properties, target, look_ahead_mask, dec_padding_mask, training):\n",
    "        logger.info(\"ENCODER STARTED\")\n",
    "        enc_output = self.encoder(properties)\n",
    "        logger.info(\"ENCODER COMPLETED\")\n",
    "        # currently the encoder output will be [batch_size, 1, d_model] i.e. sequence of size 1\n",
    "        # to ensure it is compatable with the decoder MHA first layer, \n",
    "        # we need to expand sequence length to same length as target\n",
    "        enc_output_reshaped = enc_output.unsqueeze(1).repeat(1, target.shape[1],1)\n",
    "        logger.info(f\"encoder output dimensions:{enc_output.shape}\")\n",
    "        logger.info(f\"encoder output reshaped: {enc_output_reshaped.shape}\")\n",
    "        logger.info(\"DECODER STARTED\")\n",
    "\n",
    "        dec_output = self.decoder(target, enc_output_reshaped, look_ahead_mask, dec_padding_mask)\n",
    "        ffl_output = self.final_layer(dec_output)\n",
    "\n",
    "        #####during training:\n",
    "        if training:\n",
    "            return ffl_output\n",
    "        \n",
    "        else:\n",
    "\n",
    "        ##### During inference::\n",
    "        # # the ffl output is is of shape [batch, seq_len, target_vocab_size]\n",
    "        # # the last dimension will need to be passed through a softmax to determine \n",
    "        # # the most likely token\n",
    "            # print(\"transformer output logits: \", ffl_output.shape)\n",
    "            probabilities = F.softmax(ffl_output, dim=-1)\n",
    "            # print(\"probabilities: \", probabilities.shape)\n",
    "            # To get the predicted tokens\n",
    "            predicted_tokens = torch.argmax(probabilities, dim=-1)\n",
    "            # print(\"final token\", predicted_tokens.shape)\n",
    "            return predicted_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class MoleculeDataset(Dataset):\n",
    "    def __init__(self, properties, smiles):\n",
    "        self.properties = properties\n",
    "        self.smiles = smiles\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.properties)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.properties[idx], dtype=torch.float32), torch.tensor(self.smiles[idx], dtype=torch.long)\n",
    "\n",
    "\n",
    "def preprocess_data(csv_file):\n",
    "    data = pd.read_csv(csv_file)\n",
    "    properties = data[['polararea', 'complexity', 'heavycnt', 'hbonddonor', 'hbondacc']].values\n",
    "    smiles = data['isosmiles'].values\n",
    "    print(\"length of smiles: \", smiles.shape)\n",
    "    # print(f\"smiles: {smiles}\")\n",
    "\n",
    "    # print(f\"properties: {properties}\")\n",
    "    \n",
    "    # Normalize properties\n",
    "    scaler = StandardScaler()\n",
    "    properties = scaler.fit_transform(properties)\n",
    "    \n",
    "    # Convert SMILES to a list of character indices\n",
    "    # only unique characters remain\n",
    "    # this is for creating a vocab to use to enumerate the smiles notation\n",
    "    char_to_idx = {char: idx + 3 for idx, char in enumerate(sorted(set(''.join(smiles))))}\n",
    "    char_to_idx['<pad>'] = 0\n",
    "    char_to_idx['<start>'] = 1\n",
    "    char_to_idx['<end>'] = 2\n",
    "\n",
    "    print(char_to_idx)\n",
    "    # reversing the index to character\n",
    "    idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
    "    \n",
    "    max_smiles_len = max(len(s) for s in smiles)+2 # 2 for the start and end token\n",
    "    print(\"max smiles length: \", max_smiles_len)\n",
    "    smiles_indices = [\n",
    "        [char_to_idx['<start>']] + [char_to_idx[char] for char in smi] + [char_to_idx['<end>']] + \n",
    "        [char_to_idx['<pad>']] * (max_smiles_len - len(smi) - 2)\n",
    "        for smi in smiles\n",
    "    ]\n",
    "\n",
    "    # testing the smiles indices code\n",
    "    print(\"smiles length:\",len(smiles_indices))\n",
    "    for smile_i in smiles_indices:\n",
    "        print(\"smiles example: \", smile_i)\n",
    "        print(\"smiles example length: \", len(smile_i))\n",
    "        break\n",
    "\n",
    "    return properties, smiles_indices, char_to_idx, idx_to_char, scaler\n",
    "\n",
    "properties, smiles_indices, char_to_idx, idx_to_char, scaler = preprocess_data('Pubchem.csv')\n",
    "\n",
    "train_props, test_props, train_smiles, test_smiles = train_test_split(properties, smiles_indices, test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset = MoleculeDataset(train_props, train_smiles)\n",
    "test_dataset = MoleculeDataset(test_props, test_smiles)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq_masked = torch.tensor(seq) == 0 # True if value is 0 otherwise false\n",
    "    return seq_masked.unsqueeze(1).unsqueeze(2) \n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    # creating an upper triangle of 1s\n",
    "    mask = torch.triu(torch.ones((size, size)), diagonal=1) \n",
    "    return mask.unsqueeze(0).unsqueeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = real != 0\n",
    "    # print(real.shape)\n",
    "    # print(f\"real shape: {real.shape}\")\n",
    "    # probabilities\n",
    "    # probabilities = F.softmax(pred, dim=-1)\n",
    "    # predicted_tokens = torch.argmax(probabilities, dim=-1)\n",
    "    # print(predicted_tokens[0,:])\n",
    "\n",
    "  \n",
    "    loss = nn.CrossEntropyLoss(reduction='none')(pred.transpose(1, 2), real)\n",
    "    # this crossentropyloss does both the softmax classication and the loss calculation\n",
    "    # print(\"loss shape\",loss.shape)\n",
    "    # print(\"loss:\", loss)\n",
    "\n",
    "    mask = mask.float()\n",
    "    # print(\"mask\",mask)\n",
    "    # print(\"loss matrix shape\", loss.shape)\n",
    "    # print(\"loss before mask\",loss)\n",
    "    loss *= mask\n",
    "    # print(\"loss after mask\", loss)\n",
    "\n",
    "    return torch.mean(loss)\n",
    "\n",
    "def train_model(transformer, train_loader, num_epochs, learning_rate, model_name, pretrained):\n",
    "\n",
    "    # loading pretrained models where available\n",
    "    if pretrained:\n",
    "        transformer.load_state_dict(torch.load(model_name))\n",
    "    optimizer = optim.Adam(transformer.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        transformer.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for idx_num, (properties, smiles) in enumerate(train_loader):\n",
    "   \n",
    "            # print(\"properties: \", properties.shape)\n",
    "            # print(\"smiles\", smiles.shape)\n",
    "            # print(\"smiles example\", smiles[0,:])\n",
    "            properties = properties.to(device)\n",
    "            smiles = smiles.to(device)\n",
    "            # print(\"target: \",smiles.shape)\n",
    "            # print(\"properties:\", properties.shape)\n",
    "            # print(\"smiles before masking: \",smiles)\n",
    "            \n",
    "            # print(\"smiles after masking\", enc_padding_mask)\n",
    "            # print(\"look ahead dimension:\", smiles.size(1))\n",
    "            look_ahead_mask = create_look_ahead_mask(smiles.size(1))\n",
    "            dec_padding_mask = create_padding_mask(smiles)\n",
    "\n",
    "            # print(\"look ahead mask shape\", look_ahead_mask.shape)\n",
    "            # print(\"padding mask shape\", dec_padding_mask.shape)\n",
    "            # print(\"look ahead mask: \", look_ahead_mask)\n",
    "            # print(\"padding mask\", dec_padding_mask)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predictions = transformer(properties, smiles, look_ahead_mask, dec_padding_mask, training=True)\n",
    "            # print(\"predictions\", predictions.shape)\n",
    "            # print(\"smiles: \", smiles.shape)\n",
    "            loss = loss_function(smiles[:, 1:], predictions[:, :-1])\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            print(\"batch loss\", loss.item())\n",
    "            # save model at the end of each epoch\n",
    "            torch.save(transformer.state_dict(), model_name)\n",
    "            \n",
    "        print(f'Epoch {epoch+1}, Loss: {total_loss / (idx_num + 1)}')\n",
    "\n",
    "# Initialize the model\n",
    "target_vocab_size = len(char_to_idx)\n",
    "print(\"target vocab size\", target_vocab_size)\n",
    "num_layers = 8\n",
    "enc_d_model = 5 # number of properties\n",
    "dec_d_model = 128\n",
    "enc_num_heads = 1\n",
    "dec_num_heads = 8\n",
    "enc_dff = 128 # dimension of the feed forward layer\n",
    "dec_dff = enc_dff\n",
    "pe_target = 1000 # positional encoding\n",
    "model_name = \"molecularTransformer2.pth\"\n",
    "learning_rate = 1e-5\n",
    "num_epochs = 20\n",
    "pretrained = True\n",
    "\n",
    "transformer = Transformer(num_layers, enc_d_model, dec_d_model,\n",
    "                          enc_num_heads, dec_num_heads, enc_dff, \n",
    "                          dec_dff, target_vocab_size, pe_target)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "transformer = transformer.to(device)\n",
    "\n",
    "# Train the model\n",
    "train_model(transformer, train_loader, num_epochs, learning_rate, model_name, pretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def greedy_decode(transformer, properties, max_length, start_token_idx, end_token_idx):\n",
    "      # Initialize the output sequence with the start token\n",
    "    output_sequence = torch.tensor([[start_token_idx]], dtype=torch.long).to(device)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        # Create masks\n",
    "        look_ahead_mask = create_look_ahead_mask(output_sequence.size(1))\n",
    "        dec_padding_mask = None  # No padding required during inference\n",
    "\n",
    "        # Pass through the model\n",
    "        # print(\"properties\", properties.shape)\n",
    "        # print(\"output sequence\", output_sequence.shape)\n",
    "        predictions = transformer(properties, output_sequence, look_ahead_mask, dec_padding_mask, training=False)\n",
    "        # print(\"predictions\",predictions.shape)\n",
    "        # print(\"starting tokens:\", output_sequence)\n",
    "        # print(\"predictions\", predictions)\n",
    "        # Get the predicted token for the last position\n",
    "        predictions = predictions[:, -1:]  # Focus on the last token in the sequence\n",
    "\n",
    "        # Concatenate the predicted token to the output sequence\n",
    "        output_sequence = torch.cat([output_sequence, predictions], dim=-1)\n",
    "\n",
    "        # Stop if the end token is predicted\n",
    "        if predictions.item() == end_token_idx:\n",
    "            break\n",
    "\n",
    "    return output_sequence.squeeze().tolist()\n",
    "\n",
    "def infer_and_print(transformer, test_loader, max_length, start_token_idx, end_token_idx, idx_to_char):\n",
    "\n",
    "    transformer.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx_num, (properties,smiles) in enumerate(test_loader):\n",
    "            properties = properties.to(device)\n",
    "\n",
    "            # Perform greedy decoding\n",
    "            generated_sequence = greedy_decode(transformer, properties, max_length, start_token_idx, end_token_idx)\n",
    "\n",
    "\n",
    "            generated_smiles = ''.join([idx_to_char[idx] for idx in generated_sequence if idx in idx_to_char and idx != 0])\n",
    "            smiles = list(smiles[0,:])\n",
    "            smiles = np.array(smiles)\n",
    "            # print(smiles)\n",
    "            actual_smiles = ''.join([idx_to_char[idx] for idx in smiles if idx in idx_to_char and idx != 0])\n",
    "            print(f\"Generated SMILES: {generated_smiles}\")\n",
    "            print(\"actual smiles:\", actual_smiles)\n",
    "\n",
    "# Load the trained model\n",
    "transformer.load_state_dict(torch.load(model_name))\n",
    "\n",
    "# Set up parameters for inference\n",
    "max_length =  288 # Maximum sequence length for SMILES generation\n",
    "start_token_idx = char_to_idx['<start>']\n",
    "end_token_idx = char_to_idx['<end>']\n",
    "\n",
    "# Perform inference on the test dataset\n",
    "infer_and_print(transformer, test_loader, max_length, start_token_idx, end_token_idx, idx_to_char)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
