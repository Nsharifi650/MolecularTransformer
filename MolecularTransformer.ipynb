{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "file_handler = logging.FileHandler('Logs.log')\n",
    "console_handler = logging.StreamHandler()\n",
    "file_handler.setLevel(logging.DEBUG)\n",
    "console_handler.setLevel(logging.ERROR)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s - %(message)s - Line: %(lineno)d', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "file_handler.setFormatter(formatter)\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(file_handler)\n",
    "# logger.addHandler(console_handler)\n",
    "\n",
    "logger.debug(\"This is a debug message\")\n",
    "logger.info(\"This is an info message\")\n",
    "logger.warning(\"This is a warning message\")\n",
    "logger.error(\"This is an error message\")\n",
    "logger.critical(\"This is a critical message\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MULTIHEAD ATTENTION \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        try:\n",
    "            assert d_model % num_heads == 0\n",
    "        except Exception as e:\n",
    "            logger.error(\"dimension of the embedding model is not divisable by number of heads\")\n",
    "        \n",
    "        self.d_models = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = d_model // num_heads\n",
    "\n",
    "        # The query, key, value learnable matrices\n",
    "        self.Wq = nn.Linear(d_model, d_model)\n",
    "        self.Wk = nn.Linear(d_model, d_model)\n",
    "        self.Wv = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.FCLayer = nn.Linear(d_model, d_model)\n",
    "    def split_embedding_perHead(self,x):\n",
    "        # x shape is (batch_size, seq_len, d_model)\n",
    "        (batch_size, seq_len, d_model) = x.shape\n",
    "        # logger.info(f\"multi-head; x-shape: {x.shape}\")\n",
    "        # let's reshape to (batch_size, seq_len, num_heads, depth)\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.depth)\n",
    "        logger.info(f\"Multi-head; x reshaped: {x.shape} \")\n",
    "        # changing the dimensions order to:(batch_size, num_heads, seq_len, depth)\n",
    "        x = x.permute(0,2,1,3)\n",
    "        return x\n",
    "    \n",
    "    def cal_attention(self,q,k,v,mask):\n",
    "        qk = torch.matmul(q, k.permute(0,1,3,2))\n",
    "        dk=torch.tensor(k.shape[-1], dtype=torch.float32)\n",
    "        #dk is a tensor scalar!\n",
    "        attention = qk/torch.sqrt(dk)\n",
    "\n",
    "        # print(\"CHECKING PADDING MASK CODE\")\n",
    "        # print(\"q shape\", q.shape)\n",
    "        # print(\"attention shape\", attention.shape)\n",
    "        if mask is not None:\n",
    "            attention += (mask*-1e9)\n",
    "        # print(\"attention values after masking\", attention[0,0,:,:])\n",
    "        attention_weights = F.softmax(attention, dim=-1) # should be applied along the sequence which is the 3rd dimension\n",
    "        output = torch.matmul(attention_weights, v)\n",
    "\n",
    "\n",
    "        # print(\"mask\", mask[0,0,:,:])\n",
    "        # print(\"q shape\", q.shape)\n",
    "        # print(\"mask shape\", mask.shape)\n",
    "        # print(\"attention weights shape\", attention_weights.shape)\n",
    "        # print(\"attention weights example\", attention_weights[0,0,:,:])\n",
    "        return output, attention_weights\n",
    "    \n",
    "    def forward(self, v,k,q,mask):\n",
    "        batch_size = q.shape[0]\n",
    "        # shapes for debugging\n",
    "        # print(\"v shape\", v.shape)\n",
    "        # print(\"q shape\", q.shape)\n",
    "        # print(\"k shape\", k.shape)\n",
    "        # print(\"mask shape\", mask.shape)\n",
    "        q = self.split_embedding_perHead(self.Wq(q))\n",
    "        k = self.split_embedding_perHead(self.Wk(k))\n",
    "        v = self.split_embedding_perHead(self.Wv(v))\n",
    "\n",
    "        # print(\"v shape after splitting\", v.shape)\n",
    "        # print(\"q shape after splitting\", q.shape)\n",
    "        # print(\"k shape after splitting\", k.shape)\n",
    "\n",
    "        attention,atten_weights = self.cal_attention(q,k,v,mask)\n",
    "        attention = attention.permute(0,2,1,3).contiguous()\n",
    "        attention = attention.reshape(batch_size, -1, self.d_models)\n",
    "\n",
    "        output = self.FCLayer(attention)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE ENCODER LAYER\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self,d_model,dff):\n",
    "        super(EncoderLayer,self).__init__()\n",
    "        self.FeedForwardNN = nn.Sequential(\n",
    "            nn.Linear(d_model,dff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dff,dff)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        output = self.FeedForwardNN(x)\n",
    "        logger.info(f\"encoder output dimensions {output.shape}\")\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE DECODER LAYER\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self,d_model, num_heads, dff):\n",
    "        super(DecoderLayer,self).__init__()\n",
    "        self.MultiHAttention1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.MultiHAttention2 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.FeedForwardNN = nn.Sequential(\n",
    "            nn.Linear(d_model,dff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dff,d_model)\n",
    "\n",
    "        )\n",
    "        self.layerNorm1 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.layerNorm2 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.layerNorm3 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "    def forward(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "        # print(f\"FIRST MHA WITH LOOK AHEAD MASK\")\n",
    "        attn_output1 = self.MultiHAttention1(x,x,x,look_ahead_mask)\n",
    "        attn_output1 = self.layerNorm1(x+attn_output1)\n",
    "        # print(f\"decoder input into second multihead attention layer:{attn_output1.shape}\")\n",
    "        attn_output2 = self.MultiHAttention2(enc_output, enc_output,attn_output1, padding_mask)\n",
    "        attn_output2 = self.layerNorm2(attn_output2+attn_output1)\n",
    "\n",
    "        Feedforward_output = self.FeedForwardNN(attn_output2)\n",
    "        final_output = self.layerNorm3(attn_output2+Feedforward_output)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(target_vocab_size, d_model) # d_model is the size of embedding vector\n",
    "        self.pos_encoding = self.positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, dff) for _ in range(num_layers)])\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(np.arange(position)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model)\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "        pos_encoding = angle_rads[np.newaxis, ...]\n",
    "        return torch.tensor(pos_encoding, dtype=torch.float32)\n",
    "\n",
    "    def get_angles(self, pos, i, d_model):\n",
    "        angle_rates = 1 / np.power(1000, (2 * (i // 2)) / np.float32(d_model))\n",
    "        return pos * angle_rates\n",
    "\n",
    "    def forward(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "        logger.info(f\"decoder input shape to the embedding: {x.shape}\")\n",
    "        seq_len = x.size(1)\n",
    "        x = self.embedding(x)\n",
    "        logger.info(f\"decoder input shape after embedding: {x.shape}\")\n",
    "        x *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i](x, enc_output, look_ahead_mask, padding_mask)\n",
    "\n",
    "        logger.info(f\"final decoder output shape {x.shape}\")\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORMER\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self,num_layers, enc_d_model, dec_d_model,\n",
    "                enc_num_heads, dec_num_heads, enc_dff, \n",
    "                dec_dff, target_vocab_size, pe_target):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size)\n",
    "        self.encoder = EncoderLayer(enc_d_model, enc_dff)\n",
    "        self.decoder = Decoder(num_layers, dec_d_model, dec_num_heads, dec_dff, target_vocab_size, pe_target)\n",
    "        self.final_layer = nn.Linear(dec_d_model, target_vocab_size)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, properties, target, look_ahead_mask, dec_padding_mask, training):\n",
    "        logger.info(\"ENCODER STARTED\")\n",
    "        enc_output = self.encoder(properties)\n",
    "        logger.info(\"ENCODER COMPLETED\")\n",
    "        # currently the encoder output will be [batch_size, 1, d_model] i.e. sequence of size 1\n",
    "        # to ensure it is compatable with the decoder MHA first layer, \n",
    "        # we need to expand sequence length to same length as target\n",
    "        enc_output_reshaped = enc_output.unsqueeze(1).repeat(1, target.shape[1],1)\n",
    "        logger.info(f\"encoder output dimensions:{enc_output.shape}\")\n",
    "        logger.info(f\"encoder output reshaped: {enc_output_reshaped.shape}\")\n",
    "        logger.info(\"DECODER STARTED\")\n",
    "\n",
    "        dec_output = self.decoder(target, enc_output_reshaped, look_ahead_mask, dec_padding_mask)\n",
    "        ffl_output = self.final_layer(dec_output)\n",
    "\n",
    "        #####during training:\n",
    "        if training:\n",
    "            return ffl_output\n",
    "        \n",
    "        else:\n",
    "\n",
    "        ##### During inference::\n",
    "        # # the ffl output is is of shape [batch, seq_len, target_vocab_size]\n",
    "        # # the last dimension will need to be passed through a softmax to determine \n",
    "        # # the most likely token\n",
    "            # print(\"transformer output logits: \", ffl_output.shape)\n",
    "            probabilities = F.softmax(ffl_output, dim=-1)\n",
    "            # print(\"probabilities: \", probabilities.shape)\n",
    "            # To get the predicted tokens\n",
    "            predicted_tokens = torch.argmax(probabilities, dim=-1)\n",
    "            # print(\"final token\", predicted_tokens.shape)\n",
    "            return predicted_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of smiles:  (5837,)\n",
      "{'#': 3, '%': 4, '(': 5, ')': 6, '+': 7, '-': 8, '.': 9, '/': 10, '0': 11, '1': 12, '2': 13, '3': 14, '4': 15, '5': 16, '6': 17, '7': 18, '8': 19, '9': 20, '=': 21, '@': 22, 'A': 23, 'B': 24, 'C': 25, 'F': 26, 'G': 27, 'H': 28, 'I': 29, 'K': 30, 'L': 31, 'M': 32, 'N': 33, 'O': 34, 'P': 35, 'R': 36, 'S': 37, 'T': 38, 'U': 39, 'V': 40, 'W': 41, 'Y': 42, 'Z': 43, '[': 44, '\\\\': 45, ']': 46, 'a': 47, 'b': 48, 'c': 49, 'd': 50, 'e': 51, 'f': 52, 'g': 53, 'h': 54, 'i': 55, 'l': 56, 'm': 57, 'n': 58, 'o': 59, 'p': 60, 'r': 61, 's': 62, 't': 63, 'u': 64, '<pad>': 0, '<start>': 1, '<end>': 2}\n",
      "max smiles length:  428\n",
      "smiles length: 5837\n",
      "smiles example:  [1, 25, 25, 5, 25, 33, 6, 34, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "smiles example length:  428\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class MoleculeDataset(Dataset):\n",
    "    def __init__(self, properties, smiles):\n",
    "        self.properties = properties\n",
    "        self.smiles = smiles\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.properties)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.properties[idx], dtype=torch.float32), torch.tensor(self.smiles[idx], dtype=torch.long)\n",
    "\n",
    "\n",
    "def preprocess_data(csv_file):\n",
    "    data = pd.read_csv(csv_file)\n",
    "    properties = data[['polararea', 'complexity', 'heavycnt', 'hbonddonor', 'hbondacc']].values\n",
    "    smiles = data['isosmiles'].values\n",
    "    print(\"length of smiles: \", smiles.shape)\n",
    "    # print(f\"smiles: {smiles}\")\n",
    "\n",
    "    # print(f\"properties: {properties}\")\n",
    "    \n",
    "    # Normalize properties\n",
    "    scaler = StandardScaler()\n",
    "    properties = scaler.fit_transform(properties)\n",
    "    \n",
    "    # Convert SMILES to a list of character indices\n",
    "    # only unique characters remain\n",
    "    # this is for creating a vocab to use to enumerate the smiles notation\n",
    "    char_to_idx = {char: idx + 3 for idx, char in enumerate(sorted(set(''.join(smiles))))}\n",
    "    char_to_idx['<pad>'] = 0\n",
    "    char_to_idx['<start>'] = 1\n",
    "    char_to_idx['<end>'] = 2\n",
    "\n",
    "    print(char_to_idx)\n",
    "    # reversing the index to character\n",
    "    idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
    "    \n",
    "    max_smiles_len = max(len(s) for s in smiles)+2 # 2 for the start and end token\n",
    "    print(\"max smiles length: \", max_smiles_len)\n",
    "    smiles_indices = [\n",
    "        [char_to_idx['<start>']] + [char_to_idx[char] for char in smi] + [char_to_idx['<end>']] + \n",
    "        [char_to_idx['<pad>']] * (max_smiles_len - len(smi) - 2)\n",
    "        for smi in smiles\n",
    "    ]\n",
    "\n",
    "    # testing the smiles indices code\n",
    "    print(\"smiles length:\",len(smiles_indices))\n",
    "    for smile_i in smiles_indices:\n",
    "        print(\"smiles example: \", smile_i)\n",
    "        print(\"smiles example length: \", len(smile_i))\n",
    "        break\n",
    "\n",
    "    return properties, smiles_indices, char_to_idx, idx_to_char, scaler\n",
    "\n",
    "properties, smiles_indices, char_to_idx, idx_to_char, scaler = preprocess_data('Pubchem.csv')\n",
    "\n",
    "train_props, test_props, train_smiles, test_smiles = train_test_split(properties, smiles_indices, test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset = MoleculeDataset(train_props, train_smiles)\n",
    "test_dataset = MoleculeDataset(test_props, test_smiles)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq_masked = torch.tensor(seq) == 0 # True if value is 0 otherwise false\n",
    "    return seq_masked.unsqueeze(1).unsqueeze(2) \n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    # creating an upper triangle of 1s\n",
    "    mask = torch.triu(torch.ones((size, size)), diagonal=1) \n",
    "    return mask.unsqueeze(0).unsqueeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target vocab size 65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NajibS\\AppData\\Local\\Temp\\ipykernel_23776\\2541678855.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  seq_masked = torch.tensor(seq) == 0 # True if value is 0 otherwise false\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch loss 0.06192959100008011\n",
      "batch loss 0.041502874344587326\n",
      "batch loss 0.044183939695358276\n",
      "batch loss 0.041430894285440445\n",
      "batch loss 0.039436571300029755\n",
      "batch loss 0.042118776589632034\n",
      "batch loss 0.03973597660660744\n",
      "batch loss 0.041787855327129364\n",
      "batch loss 0.03950519859790802\n",
      "batch loss 0.04352540895342827\n",
      "batch loss 0.04228944703936577\n",
      "batch loss 0.054923512041568756\n",
      "batch loss 0.047975800931453705\n",
      "batch loss 0.0509694404900074\n",
      "batch loss 0.0395393893122673\n",
      "batch loss 0.04412536323070526\n",
      "batch loss 0.04569096118211746\n",
      "batch loss 0.052282024174928665\n",
      "batch loss 0.043127045035362244\n",
      "batch loss 0.05182885751128197\n",
      "batch loss 0.04564354941248894\n",
      "batch loss 0.04701525345444679\n",
      "batch loss 0.04722653329372406\n",
      "batch loss 0.047216132283210754\n",
      "batch loss 0.03958955779671669\n",
      "batch loss 0.041406918317079544\n",
      "batch loss 0.05019775405526161\n",
      "batch loss 0.05415298789739609\n",
      "batch loss 0.05631699040532112\n",
      "batch loss 0.05288323760032654\n",
      "batch loss 0.055316485464572906\n",
      "batch loss 0.04192762449383736\n",
      "batch loss 0.03745663911104202\n",
      "batch loss 0.04238443076610565\n",
      "batch loss 0.04557149484753609\n",
      "batch loss 0.05273033678531647\n",
      "batch loss 0.045561715960502625\n",
      "batch loss 0.044298604130744934\n",
      "batch loss 0.04608692228794098\n",
      "batch loss 0.043178629130125046\n",
      "batch loss 0.0476858876645565\n",
      "batch loss 0.0383271649479866\n",
      "Epoch 1, Loss: 0.04595437566084521\n",
      "batch loss 0.05202683433890343\n",
      "batch loss 0.04675251990556717\n",
      "batch loss 0.043501950800418854\n",
      "batch loss 0.04179507493972778\n",
      "batch loss 0.04250190034508705\n",
      "batch loss 0.04117831587791443\n",
      "batch loss 0.04977461323142052\n",
      "batch loss 0.04549901559948921\n",
      "batch loss 0.05176669731736183\n",
      "batch loss 0.05131921172142029\n",
      "batch loss 0.04398869350552559\n",
      "batch loss 0.05241991579532623\n",
      "batch loss 0.04261644929647446\n",
      "batch loss 0.04432953521609306\n",
      "batch loss 0.038960058242082596\n",
      "batch loss 0.053764186799526215\n",
      "batch loss 0.044763971120119095\n",
      "batch loss 0.05515911057591438\n",
      "batch loss 0.047267504036426544\n",
      "batch loss 0.04565194621682167\n",
      "batch loss 0.049400486052036285\n",
      "batch loss 0.05193853750824928\n",
      "batch loss 0.055147673934698105\n",
      "batch loss 0.049424391239881516\n",
      "batch loss 0.03988206014037132\n",
      "batch loss 0.03841075301170349\n",
      "batch loss 0.03837563842535019\n",
      "batch loss 0.0506298765540123\n",
      "batch loss 0.03958308696746826\n",
      "batch loss 0.04417748749256134\n",
      "batch loss 0.044954754412174225\n",
      "batch loss 0.04842295125126839\n",
      "batch loss 0.04101565480232239\n",
      "batch loss 0.04402567073702812\n",
      "batch loss 0.04661647230386734\n",
      "batch loss 0.043632134795188904\n",
      "batch loss 0.044090233743190765\n",
      "batch loss 0.04675595089793205\n",
      "batch loss 0.04328921064734459\n",
      "batch loss 0.04498572275042534\n",
      "batch loss 0.0424886979162693\n",
      "batch loss 0.05272534117102623\n",
      "Epoch 2, Loss: 0.04607167361038072\n",
      "batch loss 0.05625452846288681\n",
      "batch loss 0.037396736443042755\n",
      "batch loss 0.04251670464873314\n",
      "batch loss 0.043983444571495056\n",
      "batch loss 0.03822479397058487\n",
      "batch loss 0.04732215031981468\n",
      "batch loss 0.04398360103368759\n",
      "batch loss 0.04281733185052872\n",
      "batch loss 0.04905707761645317\n",
      "batch loss 0.04353960603475571\n",
      "batch loss 0.04881798475980759\n",
      "batch loss 0.045686986297369\n",
      "batch loss 0.04210876300930977\n",
      "batch loss 0.03848309442400932\n",
      "batch loss 0.04023352265357971\n",
      "batch loss 0.04415435343980789\n",
      "batch loss 0.04072544351220131\n",
      "batch loss 0.04882584512233734\n",
      "batch loss 0.057260073721408844\n",
      "batch loss 0.054952993988990784\n",
      "batch loss 0.04622535780072212\n",
      "batch loss 0.047367632389068604\n",
      "batch loss 0.042610473930835724\n",
      "batch loss 0.04392138496041298\n",
      "batch loss 0.0500774160027504\n",
      "batch loss 0.04645784944295883\n",
      "batch loss 0.0467737540602684\n",
      "batch loss 0.055748697370290756\n",
      "batch loss 0.04837314039468765\n",
      "batch loss 0.03830806910991669\n",
      "batch loss 0.049136821180582047\n",
      "batch loss 0.036174241453409195\n",
      "batch loss 0.04975927621126175\n",
      "batch loss 0.049946703016757965\n",
      "batch loss 0.04828249663114548\n",
      "batch loss 0.04471394419670105\n",
      "batch loss 0.04876989871263504\n",
      "batch loss 0.043504297733306885\n",
      "batch loss 0.045533690601587296\n",
      "batch loss 0.049041204154491425\n",
      "batch loss 0.040876999497413635\n",
      "batch loss 0.02740880846977234\n",
      "Epoch 3, Loss: 0.04536564745718524\n",
      "batch loss 0.04367705062031746\n",
      "batch loss 0.0411883108317852\n",
      "batch loss 0.03924741968512535\n",
      "batch loss 0.044157158583402634\n",
      "batch loss 0.05005226284265518\n",
      "batch loss 0.048305779695510864\n",
      "batch loss 0.04599228501319885\n",
      "batch loss 0.040667470544576645\n",
      "batch loss 0.043973132967948914\n",
      "batch loss 0.04242128133773804\n",
      "batch loss 0.05412203073501587\n",
      "batch loss 0.03980309143662453\n",
      "batch loss 0.04915030300617218\n",
      "batch loss 0.044059667736291885\n",
      "batch loss 0.04104502499103546\n",
      "batch loss 0.052023377269506454\n",
      "batch loss 0.04272827133536339\n",
      "batch loss 0.04104389250278473\n",
      "batch loss 0.04560760781168938\n",
      "batch loss 0.04803137108683586\n",
      "batch loss 0.04459289461374283\n",
      "batch loss 0.06180815398693085\n",
      "batch loss 0.04910462349653244\n",
      "batch loss 0.05014847591519356\n",
      "batch loss 0.04233933240175247\n",
      "batch loss 0.03967837989330292\n",
      "batch loss 0.06016385555267334\n",
      "batch loss 0.059000179171562195\n",
      "batch loss 0.04217291623353958\n",
      "batch loss 0.04039224982261658\n",
      "batch loss 0.038040366023778915\n",
      "batch loss 0.04392655938863754\n",
      "batch loss 0.04042350873351097\n",
      "batch loss 0.04180971160531044\n",
      "batch loss 0.03917934373021126\n",
      "batch loss 0.04827209934592247\n",
      "batch loss 0.04356657341122627\n",
      "batch loss 0.043185409158468246\n",
      "batch loss 0.04256246238946915\n",
      "batch loss 0.048784948885440826\n",
      "batch loss 0.05614602938294411\n",
      "batch loss 0.02413899451494217\n",
      "Epoch 4, Loss: 0.045160329945030664\n",
      "batch loss 0.04120340570807457\n",
      "batch loss 0.05532868206501007\n",
      "batch loss 0.0406881645321846\n",
      "batch loss 0.05339221656322479\n",
      "batch loss 0.035659629851579666\n",
      "batch loss 0.05005324259400368\n",
      "batch loss 0.04794307053089142\n",
      "batch loss 0.057756587862968445\n",
      "batch loss 0.05408522114157677\n",
      "batch loss 0.04792739078402519\n",
      "batch loss 0.049500271677970886\n",
      "batch loss 0.04603690281510353\n",
      "batch loss 0.041107699275016785\n",
      "batch loss 0.03794820234179497\n",
      "batch loss 0.053686369210481644\n",
      "batch loss 0.04469742253422737\n",
      "batch loss 0.0404580719769001\n",
      "batch loss 0.045195214450359344\n",
      "batch loss 0.04450710862874985\n",
      "batch loss 0.04636327177286148\n",
      "batch loss 0.049998313188552856\n",
      "batch loss 0.04442782327532768\n",
      "batch loss 0.04036836326122284\n",
      "batch loss 0.04274198040366173\n",
      "batch loss 0.043005701154470444\n",
      "batch loss 0.04188788682222366\n",
      "batch loss 0.039258528500795364\n",
      "batch loss 0.037328675389289856\n",
      "batch loss 0.04403482750058174\n",
      "batch loss 0.04535118490457535\n",
      "batch loss 0.048858825117349625\n",
      "batch loss 0.04541300609707832\n",
      "batch loss 0.0451943501830101\n",
      "batch loss 0.0451875738799572\n",
      "batch loss 0.04512539133429527\n",
      "batch loss 0.05059273913502693\n",
      "batch loss 0.041751619428396225\n",
      "batch loss 0.043789248913526535\n",
      "batch loss 0.043575674295425415\n",
      "batch loss 0.04614650458097458\n",
      "batch loss 0.047592565417289734\n",
      "batch loss 0.06663958728313446\n",
      "Epoch 5, Loss: 0.04599544086626598\n",
      "batch loss 0.04148029163479805\n",
      "batch loss 0.054468851536512375\n",
      "batch loss 0.04419068619608879\n",
      "batch loss 0.042649392038583755\n",
      "batch loss 0.04523074999451637\n",
      "batch loss 0.04541526734828949\n",
      "batch loss 0.03477292135357857\n",
      "batch loss 0.04282884672284126\n",
      "batch loss 0.048799216747283936\n",
      "batch loss 0.044482674449682236\n",
      "batch loss 0.03620637208223343\n",
      "batch loss 0.039143357425928116\n",
      "batch loss 0.05441230535507202\n",
      "batch loss 0.04640337452292442\n",
      "batch loss 0.04710722342133522\n",
      "batch loss 0.042699120938777924\n",
      "batch loss 0.040341816842556\n",
      "batch loss 0.04360949248075485\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[112], line 100\u001b[0m\n\u001b[0;32m     97\u001b[0m transformer \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[112], line 61\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(transformer, train_loader, num_epochs, learning_rate, model_name, pretrained)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# print(\"look ahead mask shape\", look_ahead_mask.shape)\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# print(\"padding mask shape\", dec_padding_mask.shape)\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# print(\"look ahead mask: \", look_ahead_mask)\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# print(\"padding mask\", dec_padding_mask)\u001b[39;00m\n\u001b[0;32m     60\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 61\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproperties\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmiles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlook_ahead_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# print(\"predictions\", predictions.shape)\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# print(\"smiles: \", smiles.shape)\u001b[39;00m\n\u001b[0;32m     64\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(smiles[:, \u001b[38;5;241m1\u001b[39m:], predictions[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[101], line 28\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, properties, target, look_ahead_mask, dec_padding_mask, training)\u001b[0m\n\u001b[0;32m     25\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder output reshaped: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menc_output_reshaped\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDECODER STARTED\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 28\u001b[0m dec_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_output_reshaped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlook_ahead_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m ffl_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer(dec_output)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m#####during training:\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[100], line 33\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[1;34m(self, x, enc_output, look_ahead_mask, padding_mask)\u001b[0m\n\u001b[0;32m     30\u001b[0m x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoding[:, :seq_len, :]\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers):\n\u001b[1;32m---> 33\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdec_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlook_ahead_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal decoder output shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[99], line 22\u001b[0m, in \u001b[0;36mDecoderLayer.forward\u001b[1;34m(self, x, enc_output, look_ahead_mask, padding_mask)\u001b[0m\n\u001b[0;32m     20\u001b[0m attn_output1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayerNorm1(x\u001b[38;5;241m+\u001b[39mattn_output1)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# print(f\"decoder input into second multihead attention layer:{attn_output1.shape}\")\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m attn_output2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMultiHAttention2\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43mattn_output1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m attn_output2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayerNorm2(attn_output2\u001b[38;5;241m+\u001b[39mattn_output1)\n\u001b[0;32m     25\u001b[0m Feedforward_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mFeedForwardNN(attn_output2)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[97], line 74\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[1;34m(self, v, k, q, mask)\u001b[0m\n\u001b[0;32m     68\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_embedding_perHead(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWv(v))\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# print(\"v shape after splitting\", v.shape)\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# print(\"q shape after splitting\", q.shape)\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# print(\"k shape after splitting\", k.shape)\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m attention,atten_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcal_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m attention \u001b[38;5;241m=\u001b[39m attention\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m     76\u001b[0m attention \u001b[38;5;241m=\u001b[39m attention\u001b[38;5;241m.\u001b[39mreshape(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_models)\n",
      "Cell \u001b[1;32mIn[97], line 48\u001b[0m, in \u001b[0;36mMultiHeadAttention.cal_attention\u001b[1;34m(self, q, k, v, mask)\u001b[0m\n\u001b[0;32m     46\u001b[0m     attention \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (mask\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1e9\u001b[39m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# print(\"attention values after masking\", attention[0,0,:,:])\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m attention_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# should be applied along the sequence which is the 3rd dimension\u001b[39;00m\n\u001b[0;32m     49\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(attention_weights, v)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# print(\"mask\", mask[0,0,:,:])\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# print(\"q shape\", q.shape)\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# print(\"mask shape\", mask.shape)\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# print(\"attention weights shape\", attention_weights.shape)\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# print(\"attention weights example\", attention_weights[0,0,:,:])\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\functional.py:1858\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1856\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[0;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1858\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1860\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = real != 0\n",
    "    # print(real.shape)\n",
    "    # print(f\"real shape: {real.shape}\")\n",
    "    # probabilities\n",
    "    # probabilities = F.softmax(pred, dim=-1)\n",
    "    # predicted_tokens = torch.argmax(probabilities, dim=-1)\n",
    "    # print(predicted_tokens[0,:])\n",
    "\n",
    "  \n",
    "    loss = nn.CrossEntropyLoss(reduction='none')(pred.transpose(1, 2), real)\n",
    "    # this crossentropyloss does both the softmax classication and the loss calculation\n",
    "    # print(\"loss shape\",loss.shape)\n",
    "    # print(\"loss:\", loss)\n",
    "\n",
    "    mask = mask.float()\n",
    "    # print(\"mask\",mask)\n",
    "    # print(\"loss matrix shape\", loss.shape)\n",
    "    # print(\"loss before mask\",loss)\n",
    "    loss *= mask\n",
    "    # print(\"loss after mask\", loss)\n",
    "\n",
    "    return torch.mean(loss)\n",
    "\n",
    "def train_model(transformer, train_loader, num_epochs, learning_rate, model_name, pretrained):\n",
    "\n",
    "    # loading pretrained models where available\n",
    "    if pretrained:\n",
    "        transformer.load_state_dict(torch.load(model_name))\n",
    "    optimizer = optim.Adam(transformer.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        transformer.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for idx_num, (properties, smiles) in enumerate(train_loader):\n",
    "   \n",
    "            # print(\"properties: \", properties.shape)\n",
    "            # print(\"smiles\", smiles.shape)\n",
    "            # print(\"smiles example\", smiles[0,:])\n",
    "            properties = properties.to(device)\n",
    "            smiles = smiles.to(device)\n",
    "            # print(\"target: \",smiles.shape)\n",
    "            # print(\"properties:\", properties.shape)\n",
    "            # print(\"smiles before masking: \",smiles)\n",
    "            \n",
    "            # print(\"smiles after masking\", enc_padding_mask)\n",
    "            # print(\"look ahead dimension:\", smiles.size(1))\n",
    "            look_ahead_mask = create_look_ahead_mask(smiles.size(1))\n",
    "            dec_padding_mask = create_padding_mask(smiles)\n",
    "\n",
    "            # print(\"look ahead mask shape\", look_ahead_mask.shape)\n",
    "            # print(\"padding mask shape\", dec_padding_mask.shape)\n",
    "            # print(\"look ahead mask: \", look_ahead_mask)\n",
    "            # print(\"padding mask\", dec_padding_mask)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predictions = transformer(properties, smiles, look_ahead_mask, dec_padding_mask, training=True)\n",
    "            # print(\"predictions\", predictions.shape)\n",
    "            # print(\"smiles: \", smiles.shape)\n",
    "            loss = loss_function(smiles[:, 1:], predictions[:, :-1])\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            print(\"batch loss\", loss.item())\n",
    "            # save model at the end of each epoch\n",
    "            torch.save(transformer.state_dict(), model_name)\n",
    "            \n",
    "        print(f'Epoch {epoch+1}, Loss: {total_loss / (idx_num + 1)}')\n",
    "\n",
    "# Initialize the model\n",
    "target_vocab_size = len(char_to_idx)\n",
    "print(\"target vocab size\", target_vocab_size)\n",
    "num_layers = 8\n",
    "enc_d_model = 5 # number of properties\n",
    "dec_d_model = 128\n",
    "enc_num_heads = 1\n",
    "dec_num_heads = 8\n",
    "enc_dff = 128 # dimension of the feed forward layer\n",
    "dec_dff = enc_dff\n",
    "pe_target = 1000 # positional encoding\n",
    "model_name = \"molecularTransformer2.pth\"\n",
    "learning_rate = 1e-5\n",
    "num_epochs = 20\n",
    "pretrained = True\n",
    "\n",
    "transformer = Transformer(num_layers, enc_d_model, dec_d_model,\n",
    "                          enc_num_heads, dec_num_heads, enc_dff, \n",
    "                          dec_dff, target_vocab_size, pe_target)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "transformer = transformer.to(device)\n",
    "\n",
    "# Train the model\n",
    "train_model(transformer, train_loader, num_epochs, learning_rate, model_name, pretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated SMILES: <start>[O-].[O-].[O-].[O-].[Al+2]<end>\n",
      "actual smiles: <start>[N-]=[N+]=O<end>\n",
      "Generated SMILES: <start>C1=C(C(=C(C(=C1Cl)Cl)Cl)C(=O)Cl)Cl)C(=O)Cl<end>\n",
      "actual smiles: <start>C1[C@@H]2[C@H]3[C@@H]([C@H]1[C@H]4[C@@H]2O4)[C@]5(C(=C([C@@]3(C5(Cl)Cl)Cl)Cl)Cl)Cl<end>\n",
      "Generated SMILES: <start>CC1=CC(=C(C=C1)C(=O)OC2=CC=CC=C2)C<end>\n",
      "actual smiles: <start>CC(C)CC(=O)O[C@@H]1CC2CC[C@]1(C2(C)C)C<end>\n",
      "Generated SMILES: <start>[O-]S(=O)(=O)[O-].[Na+]<end>\n",
      "actual smiles: <start>C(=O)([O-])[O-].[Mg+2]<end>\n",
      "Generated SMILES: <start>CCCCCCCCCCCCCCO<end>\n",
      "actual smiles: <start>CN1CCC[C@H]1C2=CN=CC=C2.Cl<end>\n",
      "Generated SMILES: <start>CCCCCCCCCCCCO<end>\n",
      "actual smiles: <start>CCCCCCCC1OCC(O1)C<end>\n",
      "Generated SMILES: <start>CCCCCCCCCCCCCCCC(=O)OC(=O)CCCC(=O)C<end>\n",
      "actual smiles: <start>CC(C)(C)C1=CC=CC=C1OP(=O)([O-])OC2=CC=CC=C2<end>\n",
      "Generated SMILES: <start>CC(=O)OC(=O)C1=CC=CC=C1<end>\n",
      "actual smiles: <start>C1CN1P(=O)(N2CC2)N3CC3<end>\n",
      "Generated SMILES: <start>CC(=O)OC(=O)OCC<end>\n",
      "actual smiles: <start>C=O.C=O.C=O.C=O.C=O.[Fe]<end>\n",
      "Generated SMILES: <start>CCCCCCCCCC<end>\n",
      "actual smiles: <start>C=CC1=CC=CC=C1Cl<end>\n",
      "Generated SMILES: <start>CCCCCCCCCCCCCCCCO<end>\n",
      "actual smiles: <start>CCCCCCCOC(=O)CCCCCC<end>\n",
      "Generated SMILES: <start>C(C(C(C(C(C(CO)O)O)O)O)O)C(C(C)O)O<end>\n",
      "actual smiles: <start>C(CNCCNCCNCCNCCN)N<end>\n",
      "Generated SMILES: <start>CC(C)C(=O)O<end>\n",
      "actual smiles: <start>CC(C)(C#C)O<end>\n",
      "Generated SMILES: <start>[NH4+]<end>\n",
      "actual smiles: <start>[W]<end>\n",
      "Generated SMILES: <start>[NH4+]<end>\n",
      "actual smiles: <start>[He]<end>\n",
      "Generated SMILES: <start>C(C(=O)O)OC(=O)O.C(C(=O)O)O.C(=O)O.[Na+]<end>\n",
      "actual smiles: <start>C(C(=O)[O-])C(CC(=O)[O-])(C(=O)[O-])O.N.O.[Fe+3]<end>\n",
      "Generated SMILES: <start>CCCCCCCCCCCCO<end>\n",
      "actual smiles: <start>CCCCCCCCCC(=O)OC<end>\n",
      "Generated SMILES: <start>CCCCCCCCCC(=O)OCC<end>\n",
      "actual smiles: <start>CC/C=C\\C/C=C/CCOC(=O)C<end>\n",
      "Generated SMILES: <start>CCCCC=O<end>\n",
      "actual smiles: <start>C1=CC=NC=C1<end>\n",
      "Generated SMILES: <start>CCCCCCCC(=O)O<end>\n",
      "actual smiles: <start>CC1=CC(=CC=C1)C(=O)O<end>\n",
      "Generated SMILES: <start>CCCCCCCCCCC<end>\n",
      "actual smiles: <start>CC(C)(C)C1=CC=CC=C1<end>\n",
      "Generated SMILES: <start>CC1=CC(=C(C=C1)[N+](=O)[O-])[N+](=O)[O-](Cl)Cl<end>\n",
      "actual smiles: <start>C[C@]12CC[C@@H](C1(C)C)C[C@@H]2OC(=O)CSC#N<end>\n",
      "Generated SMILES: <start>CCCCCCCCCC(=O)OCC(=O)O<end>\n",
      "actual smiles: <start>C=CCOC(=O)C1=CC=CC=C1N<end>\n",
      "Generated SMILES: <start>CC1=CC(=C(C=C1)C2=CC=CC=C2C(=C2)C(=O)OC(=O)CC(C)C)C(C)C(C)C(C)CC(C)C<end>\n",
      "actual smiles: <start>C1C(CC2=CC=CC=C2C1C3=C(C4=CC=CC=C4OC3=O)O)C5=CC=C(C=C5)C6=CC=C(C=C6)Br<end>\n",
      "Generated SMILES: <start>C(C(F)(F)(F)F)(F)F<end>\n",
      "actual smiles: <start>C(C(F)(Br)Br)(F)(F)F<end>\n",
      "Generated SMILES: <start>CC(=O)OC(=O)CC(=O)C<end>\n",
      "actual smiles: <start>COC(=O)/C=C/C(=O)OC<end>\n",
      "Generated SMILES: <start>CCCCCCCCCC=O<end>\n",
      "actual smiles: <start>CC1=CC2=CC=CC=C2O1<end>\n",
      "Generated SMILES: <start>CCCCCCCCCCCCC(=O)OC(=O)CCC<end>\n",
      "actual smiles: <start>C1=CC(=CC=C1[N+](=O)[O-])OC2=C(C=C(C=C2)Cl)Cl<end>\n",
      "Generated SMILES: <start>CCCCCCCCCC=O<end>\n",
      "actual smiles: <start>CCCCC=C(CC)C=O<end>\n",
      "Generated SMILES: <start>CC(=O)OC(=O)C<end>\n",
      "actual smiles: <start>COP(=O)(C)OC<end>\n",
      "Generated SMILES: <start>CC(=O)[O-].CC(=O)[O-].[Na+]<end>\n",
      "actual smiles: <start>CC(C)SSSC(C)C<end>\n",
      "Generated SMILES: <start>CCCCC=O<end>\n",
      "actual smiles: <start>CCSCCCl<end>\n",
      "Generated SMILES: <start>CCCCC(=O)OCC(=O)C<end>\n",
      "actual smiles: <start>C1=CC(=CC=C1[N+](=O)[O-])F<end>\n",
      "Generated SMILES: <start>CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCO<end>\n",
      "actual smiles: <start>CCCCCCCCCCCCCCCCCC(=O)O[Al]OC(=O)CCCCCCCCCCCCCCCCC.O<end>\n",
      "Generated SMILES: <start>CCCCCCCC(=O)OCC<end>\n",
      "actual smiles: <start>CCCOC(=O)/C=C/C=C/C<end>\n",
      "Generated SMILES: <start>CCCCCCCC<end>\n",
      "actual smiles: <start>CCC1CCCCC1<end>\n",
      "Generated SMILES: <start>CCCCC<end>\n",
      "actual smiles: <start>C1CCCC1<end>\n",
      "Generated SMILES: <start>CCCCCCCCCCCC=O<end>\n",
      "actual smiles: <start>CCCCCCCC=CC=O<end>\n",
      "Generated SMILES: <start>CC(C)C(=O)OC(=O)C<end>\n",
      "actual smiles: <start>C1=CC(=CC(=C1)O)[N+](=O)[O-]<end>\n",
      "Generated SMILES: <start>CCCCCCCCO<end>\n",
      "actual smiles: <start>CC1=CC(=CC=C1)S<end>\n",
      "Generated SMILES: <start>C1=CC(=C(C(=C1)C(=O)O)C(=O)C2=CC(=C(C=C2)Cl)Cl)Cl)Cl<end>\n",
      "actual smiles: <start>CC(=CC1C(C1(C)C)C(=O)OCN2C(=O)CN(C2=O)CC#C)C<end>\n",
      "Generated SMILES: <start>CCCCCCCCCCCCCCCCCCCCCC<end>\n",
      "actual smiles: <start>CCCCCCCCCCCCCCCCCCN<end>\n",
      "Generated SMILES: <start>CCCCCCCCCC=O<end>\n",
      "actual smiles: <start>CCC(=O)C1=CC=CC=C1<end>\n",
      "Generated SMILES: <start>CCCCCCCCCCC(=O)OCC(=O)C<end>\n",
      "actual smiles: <start>CC(C)N(C(C)C)C(=O)SC/C(=C/Cl)/Cl<end>\n",
      "Generated SMILES: <start>CCCCCCCCCCC(=O)OCC<end>\n",
      "actual smiles: <start>CC/C=C/CCCC1CCCC(=O)O1<end>\n",
      "Generated SMILES: <start>[O-].[O-].[Al+]<end>\n",
      "actual smiles: <start>O=[Fe]<end>\n",
      "Generated SMILES: <start>C(C(C(=O)O)O)O.[NH](C(=O)O)O<end>\n",
      "actual smiles: <start>C([C@H]([C@H]([C@@H]([C@H](CO)O)O)O)O)O<end>\n",
      "Generated SMILES: <start>CC(=O)OC(=O)C<end>\n",
      "actual smiles: <start>CCCO[N+](=O)[O-]<end>\n",
      "Generated SMILES: <start>CCCCCCCCCCC<end>\n",
      "actual smiles: <start>C1CCCCCCCCCCC1<end>\n",
      "Generated SMILES: <start>CCCCCCCCCCCCC(=O)OC(=O)CC<end>\n",
      "actual smiles: <start>CCCCC(=O)C(CCC)SC1=C(OC=C1)C<end>\n",
      "Generated SMILES: <start>C(=O)(Cl)O<end>\n",
      "actual smiles: <start>C(=O)C(=O)O<end>\n",
      "Generated SMILES: <start>CC1=CC(=C(C=C1)C(=O)C)C2=CC=CC=C2<end>\n",
      "actual smiles: <start>C[C@@H]1CC(=O)C[C@H]2[C@]1(C[C@@H](CC2)C(=C)C)C<end>\n",
      "Generated SMILES: <start>CC(C)C=O<end>\n",
      "actual smiles: <start>CC1C(O1)C<end>\n",
      "Generated SMILES: <start>CC1=CC(=C(C=C1)NC(=O)NC)C(=O)C<end>\n",
      "actual smiles: <start>CN1C=NC2=C1C(=O)NC(=O)N2C<end>\n",
      "Generated SMILES: <start>CCCCCO<end>\n",
      "actual smiles: <start>CCCCNC<end>\n",
      "Generated SMILES: <start>CCCCCCCCCCCCCC(=O)OC(C)OCC(=O)C<end>\n",
      "actual smiles: <start>CC(C)(C)C1=CC(=C(C=C1)OP(=O)(NC)OC)Cl<end>\n",
      "Generated SMILES: <start>CCCCCCCCCCO<end>\n",
      "actual smiles: <start>C1=CC(=C(C=C1Cl)O)Cl<end>\n",
      "Generated SMILES: <start>CC1=CC=C(C=C1)C2=CC=CC=C2CC=C2<end>\n",
      "actual smiles: <start>C1=CC=C2C(=C1)C3=CC=CC4=C3C2=CC=C4<end>\n",
      "Generated SMILES: <start>CCCCC<end>\n",
      "actual smiles: <start>CC[As](Cl)Cl<end>\n",
      "Generated SMILES: <start>CCCCCCCCCCCCCCCCCCCCCCCCCCCC<end>\n",
      "actual smiles: <start>CCCCCCCC/C=C\\CCCCCCCC(=O)[O-].[Na+]<end>\n",
      "Generated SMILES: <start>C(C(=O)O)Cl<end>\n",
      "actual smiles: <start>C(CO)C#N<end>\n",
      "Generated SMILES: <start>CC(=O)C<end>\n",
      "actual smiles: <start>COC=C<end>\n",
      "Generated SMILES: <start>CCCCCCCCCCC(=O)OC<end>\n",
      "actual smiles: <start>CC/C=C\\CCCCCOC(=O)C<end>\n",
      "Generated SMILES: <start>CCCCCCCCCCCCCCCC(=O)O<end>\n",
      "actual smiles: <start>CCCCCCCC(CCCC(=O)OCC(CO)O)O<end>\n",
      "Generated SMILES: <start>CCCCCCCCCCCCC<end>\n",
      "actual smiles: <start>CCCCCCCCC=COC<end>\n",
      "Generated SMILES: <start>CCCCCCCC(=O)OCCC<end>\n",
      "actual smiles: <start>C/C=C/C=C/COC(=O)C(C)C<end>\n",
      "Generated SMILES: <start>C1=CC(=C(C=C1)NC(=O)NC2=CC=C(C=C2)O)O<end>\n",
      "actual smiles: <start>CC(C(=O)NCCC1=CC=C(C=C1)O)O<end>\n",
      "Generated SMILES: <start>CCCCCCC(=O)OCC<end>\n",
      "actual smiles: <start>C1=CC(=C(C(=C1)Cl)[N+](=O)[O-])Cl<end>\n",
      "Generated SMILES: <start>CC(=O)C(=O)C<end>\n",
      "actual smiles: <start>CC(C(=O)C)SC<end>\n",
      "Generated SMILES: <start>C(=O)(Cl)(Cl)O<end>\n",
      "actual smiles: <start>C(C(=O)[O-])F.[Na+]<end>\n",
      "Generated SMILES: <start>CCCCCCC(=O)OC<end>\n",
      "actual smiles: <start>CCCOC(=O)C(C)C<end>\n",
      "Generated SMILES: <start>CCCCCCC(=O)OCC<end>\n",
      "actual smiles: <start>CCC(C)(C(=O)OCC)O<end>\n",
      "Generated SMILES: <start>[O-][O-][O-]S(=O)(=O)[O-].[O-].[O-].[Ca+2]<end>\n",
      "actual smiles: <start>[N+](=O)([O-])[O-].[N+](=O)([O-])[O-].[Ni+2]<end>\n",
      "Generated SMILES: <start>CCCCCCCCCCCC<end>\n",
      "actual smiles: <start>CCC=CCC=CCCO<end>\n",
      "Generated SMILES: <start>CC(C(C(C(C(=O)O)O)O)O)O.C(C(CO)O)O.C(C(=O)O)O.C(C(C(=O)O)O)C(C(=O)O.C(C(C(=O)O)O)C(C(C(=O)O)O)C(C(C(=O)O)O)C<end>\n",
      "actual smiles: <start>CN1CCCC1C2=CN=CC=C2.[C@@H]([C@H](C(=O)O)O)(C(=O)O)O.[C@@H]([C@H](C(=O)O)O)(C(=O)O)O<end>\n",
      "Generated SMILES: <start>CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCO<end>\n",
      "actual smiles: <start>CCCCCCCCCCCCCCCCCC(=O)[O-].CCCCCCCCCCCCCCCCCC(=O)[O-].[Pb+2]<end>\n",
      "Generated SMILES: <start>CCCCCCCCCC(=O)OCCC(=O)C<end>\n",
      "actual smiles: <start>CC(=CCC/C(=C/COC(=O)C)/C)C<end>\n",
      "Generated SMILES: <start>CCCCCCCCCC(=O)OC(=O)CCC<end>\n",
      "actual smiles: <start>CC/C=C\\CC1[C@H](CCC1=O)CC(=O)OC<end>\n",
      "Generated SMILES: <start>CCCCCCCCCC(=O)OC<end>\n",
      "actual smiles: <start>CCC/C=C/COC(=O)CCC<end>\n",
      "Generated SMILES: <start>CCCCCCCC(=O)OCC<end>\n",
      "actual smiles: <start>CCN(CC)C(=O)CC(=O)C<end>\n",
      "Generated SMILES: <start>CC1=CC=C(C=C1)C(C)O<end>\n",
      "actual smiles: <start>CC(=CCC/C(=C\\CO)/C)C<end>\n",
      "Generated SMILES: <start>CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCO<end>\n",
      "actual smiles: <start>CCCCCCCCCCCCCCCC(=O)OCC(COP(=O)([O-])OCC[N+](C)(C)C)OC(=O)CCCCCCCC=CCC=CCCCCC<end>\n",
      "Generated SMILES: <start>CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCO<end>\n",
      "actual smiles: <start>C1=CC=C(C=C1)O[Si](OC2=CC=CC=C2)(OC3=CC=CC=C3)OC4=CC=CC=C4<end>\n",
      "Generated SMILES: <start>CCCCCCCCCCCCCC(=O)OCC<end>\n",
      "actual smiles: <start>CC(C)CC(=O)OC/C=C/C1=CC=CC=C1<end>\n",
      "Generated SMILES: <start>C(=O)[O-].[Cr].[Na+]<end>\n",
      "actual smiles: <start>[O-][Mo](=O)(=O)[O-].[Na+].[Na+]<end>\n",
      "Generated SMILES: <start>C1=C(C(=C(C(=C1Cl)Cl)Cl)C(=O)Cl)Cl)C(=O)Cl<end>\n",
      "actual smiles: <start>C1[C@@H]2[C@@H]3[C@H]([C@@H]1[C@@H]4[C@H]2O4)[C@]5(C(=C([C@@]3(C5(Cl)Cl)Cl)Cl)Cl)Cl<end>\n",
      "Generated SMILES: <start>CCCCCCO<end>\n",
      "actual smiles: <start>C1=CC(=CC=C1N)F<end>\n",
      "Generated SMILES: <start>CC1=CC(=C(C=C1)NC(=O)NC(=O)NC)NC(=O)C<end>\n",
      "actual smiles: <start>C1=CC(=CC=C1N)S(=O)(=O)C2=CC=C(C=C2)N<end>\n",
      "Generated SMILES: <start>CCCCCCC<end>\n",
      "actual smiles: <start>C1=CC=C(C=C1)I<end>\n",
      "Generated SMILES: <start>CCCCCCCCCCCCCCCCCCCCCCCCO<end>\n",
      "actual smiles: <start>CCCCCCOC(=O)CCCCCCCC(=O)OCCCCCC<end>\n",
      "Generated SMILES: <start>CCCCCCCCCCCCC<end>\n",
      "actual smiles: <start>CCCCCCCC/C=C/C=O<end>\n",
      "Generated SMILES: <start>CCCCCCCCCCCC<end>\n",
      "actual smiles: <start>C1=C(C(=C(C(=C1Cl)Cl)Cl)Cl)Cl<end>\n",
      "Generated SMILES: <start>C1=CC=C(C=C1)C(=O)O<end>\n",
      "actual smiles: <start>CCC1CC(=C(C1=O)O)C<end>\n",
      "Generated SMILES: <start>C(C(=O)O)O<end>\n",
      "actual smiles: <start>CC(C(=O)O)O<end>\n",
      "Generated SMILES: <start>CC1=CC=C(C=C1)NC(=O)C2=CC=CC=C2<end>\n",
      "actual smiles: <start>CC(=O)NC1=CC=C(C=C1)C(=O)CCl<end>\n",
      "Generated SMILES: <start>CCCCCCCCCCCCCCC(=O)OC(C(=O)O)OCC(=O)OCCCC(=O)O<end>\n",
      "actual smiles: <start>C1=CC=C(C=C1)COCC(C(=O)O)N(CCN(CCN(CC(=O)O)CC(=O)O)CC(=O)O)CC(=O)O<end>\n",
      "Generated SMILES: <start>CCC(=O)CC(=O)C<end>\n",
      "actual smiles: <start>CC(CC(=O)C)SC<end>\n",
      "Generated SMILES: <start>C(=O)[O-].[Cr+2]<end>\n",
      "actual smiles: <start>[O-][Si](=O)[O-].[Na+].[Na+]<end>\n",
      "Generated SMILES: <start>CC(C)(C)O<end>\n",
      "actual smiles: <start>CC(=C)C(=O)O<end>\n",
      "Generated SMILES: <start>C(C(=O)O)O<end>\n",
      "actual smiles: <start>C(C(=O)O)S<end>\n",
      "Generated SMILES: <start>CCCCCCO<end>\n",
      "actual smiles: <start>CC(C)CC(C)N<end>\n",
      "Generated SMILES: <start>CC(=O)[O-].[Cr+2].[Cl-]<end>\n",
      "actual smiles: <start>CC(=O)[O-].CC(=O)[O-].[Cd+2]<end>\n",
      "Generated SMILES: <start>CCCCCCCCCCCCCC<end>\n",
      "actual smiles: <start>C1=CC=C(C=C1)CCCCCO<end>\n",
      "Generated SMILES: <start>CC1=CC(=C(C=C1)C(=O)OC)C(C)C<end>\n",
      "actual smiles: <start>C/C(=C\\C/C=C(\\C)/C=C)/COC(=O)C<end>\n",
      "Generated SMILES: <start>CCCCCCCCCCCC(=O)OC(=O)OCCC(=O)OC<end>\n",
      "actual smiles: <start>CCN(CC)C1=NC(=CC(=N1)OP(=S)(OCC)OCC)C<end>\n",
      "Generated SMILES: <start>CC(CCCO)CO<end>\n",
      "actual smiles: <start>C1=CC(=CC(=C1)O)O<end>\n",
      "Generated SMILES: <start>CCCCCCCCCCCCCCO<end>\n",
      "actual smiles: <start>CCCCCC(=O)OCCC(CCC)S<end>\n",
      "Generated SMILES: <start>CCCCCCCCC=O<end>\n",
      "actual smiles: <start>CC1=NC=C(C=C1)C=C<end>\n",
      "Generated SMILES: <start>[O-].[O-](F)(F)(F)(F)(F)(F)(F)(F)(F)(F)F.[Fe+2]<end>\n",
      "actual smiles: <start>C(C(C(F)(F)F)(F)F)(C(F)(F)F)(F)F<end>\n",
      "Generated SMILES: <start>CC(C)S<end>\n",
      "actual smiles: <start>CC(C)N<end>\n",
      "Generated SMILES: <start>C(=O)(C(=O)[O-])[O-].[Na+]<end>\n",
      "actual smiles: <start>O[As](=O)([O-])[O-].[Na+].[Na+]<end>\n",
      "Generated SMILES: <start>CCCC(=O)OCCC(C)C<end>\n",
      "actual smiles: <start>C/C=C\\C=C\\C(=O)OC<end>\n",
      "Generated SMILES: <start>CCCCCCCCCCCCCC<end>\n",
      "actual smiles: <start>C[C@@H]1CC[C@H](C[C@@H]1O)C(C)C<end>\n",
      "Generated SMILES: <start>CC(=O)Cl<end>\n",
      "actual smiles: <start>O=P(Cl)(Cl)Cl<end>\n",
      "Generated SMILES: <start>C(C(C(=O)O)O)O.[C@H](CO)O<end>\n",
      "actual smiles: <start>C(O)[P+](CO)(CO)CO.[Cl-]<end>\n",
      "Generated SMILES: <start>CCC(=O)CC(C)O<end>\n",
      "actual smiles: <start>COC(=O)C(Cl)Cl<end>\n",
      "Generated SMILES: <start>CC1=CC=C(C=C1)C(C)C<end>\n",
      "actual smiles: <start>CC(=CC/C=C(\\C)/C=C)C<end>\n",
      "Generated SMILES: <start>CC(C)C(C)(C)(C)(C)(C)(C)O)O.[NH](C(C)O)C(=O)[O-]<end>\n",
      "actual smiles: <start>C1=CC=C(C=C1)NC(=O)NC2=CN=NS2<end>\n",
      "Generated SMILES: <start>C1=CC(=C(C(=C1Cl)Cl)Cl)C(=O)Cl)Cl<end>\n",
      "actual smiles: <start>C[C@@]12CCCC(C1=CC(=O)O2)(C)C<end>\n",
      "Generated SMILES: <start>C1=CC(=C2=CC(=C1)C2=CC(=C2)C(=C(C=C2)C)C)C(C)C(=C)C)C(C)C<end>\n",
      "actual smiles: <start>C[C@H](/C=C/[C@H](C)C(C)C)[C@H]1CC[C@@H]2[C@@]1(CC[C@H]3C2=CC=C4[C@@]3(CC[C@@H](C4)O)C)C<end>\n",
      "Generated SMILES: <start>CC1=CC=C(C=C1)C(C)O<end>\n",
      "actual smiles: <start>CC(=C)C1CCC(CC1)(C)O<end>\n",
      "Generated SMILES: <start>CCCCCCCCC(=O)OCCC<end>\n",
      "actual smiles: <start>CCCCCCC#CC(=O)OCC<end>\n",
      "Generated SMILES: <start>CC(C)OC(=O)OC(=O)OC(=O)CC(=O)C<end>\n",
      "actual smiles: <start>C(C(=O)[O-])N(CC(=O)[O-])CC(=O)[O-].O.[Na+].[Na+].[Na+]<end>\n",
      "Generated SMILES: <start>CCCCCCO<end>\n",
      "actual smiles: <start>CC(=CCO)C<end>\n",
      "Generated SMILES: <start>CCCCCO<end>\n",
      "actual smiles: <start>CC(C)(C)CO<end>\n",
      "Generated SMILES: <start>CCCCCCCC(=O)OC(=O)OCC(=O)C<end>\n",
      "actual smiles: <start>C1COCCN1SC2=NC3=CC=CC=C3S2<end>\n",
      "Generated SMILES: <start>CCCCCCCCCCCCCC<end>\n",
      "actual smiles: <start>C[C@H]1CC[C@H]([C@H](C1)O)C(C)C<end>\n",
      "Generated SMILES: <start>CC1=CC=C(C=C1)C(=O)O<end>\n",
      "actual smiles: <start>CC(=O)NC1=CC=C(C=C1)Cl<end>\n",
      "Generated SMILES: <start>CC1=CC(=C(C=C1)C(=O)OC2=CC(=C(C=C2)C(=O)C)C)C(C)C(C)C)C<end>\n",
      "actual smiles: <start>CC1(C(C1C(=O)OC(C#N)C2=CC(=C(C=C2)F)OC3=CC=CC=C3)C=C(Cl)Cl)C<end>\n",
      "Generated SMILES: <start>[Cl-].[Cl-]<end>\n",
      "actual smiles: <start>BrI<end>\n",
      "Generated SMILES: <start>CC1=CC(=C(C=C1)C)C(C)C(C)C(C)C(C)C<end>\n",
      "actual smiles: <start>CC(=CCCC(=CCCC(C)(C=C)O)C)C<end>\n",
      "Generated SMILES: <start>CCCCCCCCCCC(=O)OC<end>\n",
      "actual smiles: <start>CC/C=C/CCOC(=O)C(C)CC<end>\n",
      "Generated SMILES: <start>C(C(C(=O)O)O)O.C(C(=O)O)O.C(C(=O)O)C(=O)O<end>\n",
      "actual smiles: <start>C(C(=O)O)C(CC(=O)O)(C(=O)O)O.[Fe+3]<end>\n",
      "Generated SMILES: <start>CCCCCCC(=O)O<end>\n",
      "actual smiles: <start>C1CCC(CC1)C(=O)O<end>\n",
      "Generated SMILES: <start>CCCCC=O<end>\n",
      "actual smiles: <start>CCCCC=O<end>\n",
      "Generated SMILES: <start>CC1=CC(=C(C=C1)C(C)C)C(C)C(C)C(C)O<end>\n",
      "actual smiles: <start>CC1=CC=CC=C1NC(=NC2=CC=CC=C2C)N<end>\n",
      "Generated SMILES: <start>CCCCCCCCOC(=O)C<end>\n",
      "actual smiles: <start>CCCCCCOC(=O)Cl<end>\n",
      "Generated SMILES: <start>CC(C)O<end>\n",
      "actual smiles: <start>CC(=O)CS<end>\n",
      "Generated SMILES: <start>CCCCCCCCCO<end>\n",
      "actual smiles: <start>CC/C=C/C=C/CO<end>\n",
      "Generated SMILES: <start>C(Cl)(F)F<end>\n",
      "actual smiles: <start>[NH4+].[I-]<end>\n",
      "Generated SMILES: <start>C(=O)[O-].[Cr+2]<end>\n",
      "actual smiles: <start>CCO[N+](=O)[O-]<end>\n",
      "Generated SMILES: <start>CCCCCCCCC(=O)OC<end>\n",
      "actual smiles: <start>CCCCCCOC(=O)C=C<end>\n",
      "Generated SMILES: <start>CCCCCCCCCCCC<end>\n",
      "actual smiles: <start>C1=CC=C(C=C1)/C=C/CO<end>\n",
      "Generated SMILES: <start>C(C(=O)O)OC(=O)C(=O)O<end>\n",
      "actual smiles: <start>CCOP(=O)(C(=O)N)[O-].[NH4+]<end>\n",
      "Generated SMILES: <start>CCCCCCCCCCCC(=O)OCCC<end>\n",
      "actual smiles: <start>CC(C)C(=O)OC/C=C/C1=CC=CC=C1<end>\n",
      "Generated SMILES: <start>CCCCCCCCCCCCCCC<end>\n",
      "actual smiles: <start>C1=CC=C2C=C3C=CC=CC3=CC2=C1<end>\n",
      "Generated SMILES: <start>CC1=CC(=C(C=C1)C)C(C)C(C)C)O<end>\n",
      "actual smiles: <start>CC(=CCC/C(=C/C(C)(C)O)/C)C<end>\n",
      "Generated SMILES: <start>CCCCCCCCCCC=O<end>\n",
      "actual smiles: <start>CC(C)C1=CC=C(C=C1)C=O<end>\n",
      "Generated SMILES: <start>CCCCCCCCCC=O<end>\n",
      "actual smiles: <start>CC(C=O)C1=CC=CC=C1<end>\n",
      "Generated SMILES: <start>CCCCCCOC(=O)C<end>\n",
      "actual smiles: <start>CCOP(=O)(CC)OCC<end>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[113], line 87\u001b[0m\n\u001b[0;32m     84\u001b[0m end_token_idx \u001b[38;5;241m=\u001b[39m char_to_idx[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<end>\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# Perform inference on the test dataset\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m \u001b[43minfer_and_print\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_token_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_token_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx_to_char\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[113], line 64\u001b[0m, in \u001b[0;36minfer_and_print\u001b[1;34m(transformer, test_loader, max_length, start_token_idx, end_token_idx, idx_to_char)\u001b[0m\n\u001b[0;32m     61\u001b[0m properties \u001b[38;5;241m=\u001b[39m properties\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Perform greedy decoding\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[43mgreedy_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperties\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_token_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_token_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Convert the generated token indices to SMILES characters\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# print(generated_sequence.shape)\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# print(smiles.shape)\u001b[39;00m\n\u001b[0;32m     70\u001b[0m generated_smiles \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([idx_to_char[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m generated_sequence \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m idx_to_char \u001b[38;5;129;01mand\u001b[39;00m idx \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[1;32mIn[113], line 29\u001b[0m, in \u001b[0;36mgreedy_decode\u001b[1;34m(transformer, properties, max_length, start_token_idx, end_token_idx)\u001b[0m\n\u001b[0;32m     24\u001b[0m dec_padding_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# No padding required during inference\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Pass through the model\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# print(\"properties\", properties.shape)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# print(\"output sequence\", output_sequence.shape)\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproperties\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlook_ahead_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# print(\"predictions\",predictions.shape)\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# print(\"starting tokens:\", output_sequence)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# print(\"predictions\", predictions)\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Get the predicted token for the last position\u001b[39;00m\n\u001b[0;32m     34\u001b[0m predictions \u001b[38;5;241m=\u001b[39m predictions[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# Focus on the last token in the sequence\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[101], line 18\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, properties, target, look_ahead_mask, dec_padding_mask, training)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, properties, target, look_ahead_mask, dec_padding_mask, training):\n\u001b[0;32m     17\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mENCODER STARTED\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m     enc_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproperties\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mENCODER COMPLETED\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# currently the encoder output will be [batch_size, 1, d_model] i.e. sequence of size 1\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# to ensure it is compatable with the decoder MHA first layer, \u001b[39;00m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# we need to expand sequence length to same length as target\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[98], line 13\u001b[0m, in \u001b[0;36mEncoderLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[0;32m     12\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mFeedForwardNN(x)\n\u001b[1;32m---> 13\u001b[0m     \u001b[43mlogger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoder output dimensions \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\NajibS\\.conda\\envs\\RAG_env\\Lib\\logging\\__init__.py:1489\u001b[0m, in \u001b[0;36mLogger.info\u001b[1;34m(self, msg, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1480\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1481\u001b[0m \u001b[38;5;124;03mLog 'msg % args' with severity 'INFO'.\u001b[39;00m\n\u001b[0;32m   1482\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1486\u001b[0m \u001b[38;5;124;03mlogger.info(\"Houston, we have a %s\", \"interesting problem\", exc_info=1)\u001b[39;00m\n\u001b[0;32m   1487\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39misEnabledFor(INFO):\n\u001b[1;32m-> 1489\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log\u001b[49m\u001b[43m(\u001b[49m\u001b[43mINFO\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\NajibS\\.conda\\envs\\RAG_env\\Lib\\logging\\__init__.py:1634\u001b[0m, in \u001b[0;36mLogger._log\u001b[1;34m(self, level, msg, args, exc_info, extra, stack_info, stacklevel)\u001b[0m\n\u001b[0;32m   1631\u001b[0m         exc_info \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n\u001b[0;32m   1632\u001b[0m record \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmakeRecord(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, level, fn, lno, msg, args,\n\u001b[0;32m   1633\u001b[0m                          exc_info, func, extra, sinfo)\n\u001b[1;32m-> 1634\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\NajibS\\.conda\\envs\\RAG_env\\Lib\\logging\\__init__.py:1644\u001b[0m, in \u001b[0;36mLogger.handle\u001b[1;34m(self, record)\u001b[0m\n\u001b[0;32m   1637\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1638\u001b[0m \u001b[38;5;124;03mCall the handlers for the specified record.\u001b[39;00m\n\u001b[0;32m   1639\u001b[0m \n\u001b[0;32m   1640\u001b[0m \u001b[38;5;124;03mThis method is used for unpickled records received from a socket, as\u001b[39;00m\n\u001b[0;32m   1641\u001b[0m \u001b[38;5;124;03mwell as those created locally. Logger-level filtering is applied.\u001b[39;00m\n\u001b[0;32m   1642\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1643\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisabled) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter(record):\n\u001b[1;32m-> 1644\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallHandlers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\NajibS\\.conda\\envs\\RAG_env\\Lib\\logging\\__init__.py:1706\u001b[0m, in \u001b[0;36mLogger.callHandlers\u001b[1;34m(self, record)\u001b[0m\n\u001b[0;32m   1704\u001b[0m     found \u001b[38;5;241m=\u001b[39m found \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1705\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m record\u001b[38;5;241m.\u001b[39mlevelno \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m hdlr\u001b[38;5;241m.\u001b[39mlevel:\n\u001b[1;32m-> 1706\u001b[0m         \u001b[43mhdlr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1707\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m c\u001b[38;5;241m.\u001b[39mpropagate:\n\u001b[0;32m   1708\u001b[0m     c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m    \u001b[38;5;66;03m#break out\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\NajibS\\.conda\\envs\\RAG_env\\Lib\\logging\\__init__.py:978\u001b[0m, in \u001b[0;36mHandler.handle\u001b[1;34m(self, record)\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macquire()\n\u001b[0;32m    977\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 978\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    980\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\NajibS\\.conda\\envs\\RAG_env\\Lib\\logging\\__init__.py:1230\u001b[0m, in \u001b[0;36mFileHandler.emit\u001b[1;34m(self, record)\u001b[0m\n\u001b[0;32m   1228\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open()\n\u001b[0;32m   1229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream:\n\u001b[1;32m-> 1230\u001b[0m     \u001b[43mStreamHandler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\NajibS\\.conda\\envs\\RAG_env\\Lib\\logging\\__init__.py:1114\u001b[0m, in \u001b[0;36mStreamHandler.emit\u001b[1;34m(self, record)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[38;5;66;03m# issue 35046: merged two stream.writes into one.\u001b[39;00m\n\u001b[0;32m   1113\u001b[0m     stream\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mterminator)\n\u001b[1;32m-> 1114\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1115\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRecursionError\u001b[39;00m:  \u001b[38;5;66;03m# See issue 36272\u001b[39;00m\n\u001b[0;32m   1116\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\NajibS\\.conda\\envs\\RAG_env\\Lib\\logging\\__init__.py:1094\u001b[0m, in \u001b[0;36mStreamHandler.flush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflush\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1094\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1096\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelease()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def greedy_decode(transformer, properties, max_length, start_token_idx, end_token_idx):\n",
    "    \"\"\"\n",
    "    Perform greedy decoding on the Transformer model.\n",
    "    \n",
    "    Args:\n",
    "        transformer: The trained Transformer model.\n",
    "        properties: The input physical properties (features).\n",
    "        max_length: The maximum length of the sequence to generate.\n",
    "        start_token_idx: The index of the start token in the vocabulary.\n",
    "        end_token_idx: The index of the end token in the vocabulary.\n",
    "\n",
    "    Returns:\n",
    "        A list of generated token indices representing the predicted sequence.\n",
    "    \"\"\"\n",
    "    # Initialize the output sequence with the start token\n",
    "    output_sequence = torch.tensor([[start_token_idx]], dtype=torch.long).to(device)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        # Create masks\n",
    "        look_ahead_mask = create_look_ahead_mask(output_sequence.size(1))\n",
    "        dec_padding_mask = None  # No padding required during inference\n",
    "\n",
    "        # Pass through the model\n",
    "        # print(\"properties\", properties.shape)\n",
    "        # print(\"output sequence\", output_sequence.shape)\n",
    "        predictions = transformer(properties, output_sequence, look_ahead_mask, dec_padding_mask, training=False)\n",
    "        # print(\"predictions\",predictions.shape)\n",
    "        # print(\"starting tokens:\", output_sequence)\n",
    "        # print(\"predictions\", predictions)\n",
    "        # Get the predicted token for the last position\n",
    "        predictions = predictions[:, -1:]  # Focus on the last token in the sequence\n",
    "\n",
    "        # Concatenate the predicted token to the output sequence\n",
    "        output_sequence = torch.cat([output_sequence, predictions], dim=-1)\n",
    "\n",
    "        # Stop if the end token is predicted\n",
    "        if predictions.item() == end_token_idx:\n",
    "            break\n",
    "\n",
    "    return output_sequence.squeeze().tolist()\n",
    "\n",
    "def infer_and_print(transformer, test_loader, max_length, start_token_idx, end_token_idx, idx_to_char):\n",
    "    \"\"\"\n",
    "    Perform inference on the test dataset and print the generated sequences.\n",
    "    \n",
    "    Args:\n",
    "        transformer: The trained Transformer model.\n",
    "        test_loader: DataLoader containing the test data (properties).\n",
    "        max_length: The maximum sequence length for generation.\n",
    "        start_token_idx: The index of the start token.\n",
    "        end_token_idx: The index of the end token.\n",
    "        idx_to_char: Dictionary to convert indices back to SMILES characters.\n",
    "    \"\"\"\n",
    "    transformer.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx_num, (properties,smiles) in enumerate(test_loader):\n",
    "            properties = properties.to(device)\n",
    "\n",
    "            # Perform greedy decoding\n",
    "            generated_sequence = greedy_decode(transformer, properties, max_length, start_token_idx, end_token_idx)\n",
    "\n",
    "            # Convert the generated token indices to SMILES characters\n",
    "            # print(generated_sequence.shape)\n",
    "            # print(smiles.shape)\n",
    "\n",
    "            generated_smiles = ''.join([idx_to_char[idx] for idx in generated_sequence if idx in idx_to_char and idx != 0])\n",
    "            smiles = list(smiles[0,:])\n",
    "            smiles = np.array(smiles)\n",
    "            # print(smiles)\n",
    "            actual_smiles = ''.join([idx_to_char[idx] for idx in smiles if idx in idx_to_char and idx != 0])\n",
    "            print(f\"Generated SMILES: {generated_smiles}\")\n",
    "            print(\"actual smiles:\", actual_smiles)\n",
    "\n",
    "# Load the trained model\n",
    "transformer.load_state_dict(torch.load(model_name))\n",
    "\n",
    "# Set up parameters for inference\n",
    "max_length =  288 # Maximum sequence length for SMILES generation\n",
    "start_token_idx = char_to_idx['<start>']\n",
    "end_token_idx = char_to_idx['<end>']\n",
    "\n",
    "# Perform inference on the test dataset\n",
    "infer_and_print(transformer, test_loader, max_length, start_token_idx, end_token_idx, idx_to_char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
